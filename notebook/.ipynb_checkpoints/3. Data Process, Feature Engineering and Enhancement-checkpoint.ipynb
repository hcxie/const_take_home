{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4678662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, TweedieRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d189cc",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d254ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv(\"../data/dengue_features_train.csv\")\n",
    "train_label = pd.read_csv(\"../data/dengue_labels_train.csv\")\n",
    "test_data = pd.read_csv(\"../data/dengue_features_test.csv\")\n",
    "\n",
    "train_data_sj = train_data.loc[train_data[\"city\"]==\"sj\",:].copy()\n",
    "train_data_sj.drop(columns = [\"city\"], inplace = True)\n",
    "train_label_sj = train_label.loc[train_label[\"city\"]==\"sj\",:].copy()\n",
    "train_label_sj.drop(columns = [\"city\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bf55b",
   "metadata": {},
   "source": [
    "# Dengue Categorical encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f476a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dengue_cat_encoder(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class serves 3 functions:\n",
    "    1. one-hot encode \"month\" variable\n",
    "    2. target encode \"weekofyear\" variable\n",
    "    3. remove column: \"week_start_date\", \"year\"\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.target_encoder_dict = None\n",
    "        self.month_list = None\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # ---------------------input check-------------------------------\n",
    "        for column in [\"weekofyear\", \"week_start_date\"]:\n",
    "            if column not in X.columns:\n",
    "                raise ValueError(\"{} column must exist in the data\".format(column))\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        print(\"Fitting Cat_encoder\")\n",
    "        \n",
    "        # create a dict to store the target encoding values for week of year\n",
    "        weekofyear = X.loc[:,[\"weekofyear\"]].copy()\n",
    "        weekofyear.loc[:, \"total_cases\"] = y.values\n",
    "        self.target_encoder_dict = weekofyear.groupby(\"weekofyear\")[\"total_cases\"].mean().to_dict()\n",
    "        \n",
    "        # just in case, if we accdentily split a small portion of data and do not have the\n",
    "        # all week numbers in a year, we impute it with mean cases each week from the training\n",
    "        # data\n",
    "        mean_cases = np.mean(list(self.target_encoder_dict.values()))\n",
    "        week_within_a_year = int(365/7)+1\n",
    "        for i in range(1, week_within_a_year+1):\n",
    "            if i not in self.target_encoder_dict:\n",
    "                self.target_encoder_dict[i] = mean_cases\n",
    "                \n",
    "        # check the month value in the training data\n",
    "        # store it in the class for future verificaiton\n",
    "        \n",
    "        print(\"Fitting Cat_encoder completed\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # ---------------------input check-------------------------------\n",
    "        for column in [\"weekofyear\", \"week_start_date\", \"year\"]:\n",
    "            if column not in X.columns:\n",
    "                raise ValueError(\"{} column must exist in the data\".format(column))\n",
    "                \n",
    "        if not self.target_encoder_dict:\n",
    "            raise ValueError(\"Please fit the training data first\")\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        print(\"Transforming data using Cat_encoder\")\n",
    "        \n",
    "        # extract month and one hot encodeing it\n",
    "        temp_date = pd.to_datetime(X.loc[:, \"week_start_date\"])\n",
    "        X.loc[:, \"month\"] = temp_date.dt.month.astype(object)\n",
    "        new_X = X.drop(columns=[\"week_start_date\", \"year\"])\n",
    "        dummy_month = pd.get_dummies(new_X[\"month\"], drop_first = True)\n",
    "        \n",
    "        # when we first fit the training data, record the month we included\n",
    "        if not self.month_list:\n",
    "            self.month_list = set(dummy_month.columns)\n",
    "        \n",
    "        # check with month_list to make sure all month names appear in the data\n",
    "        # if we transform the test data and a month does not exist, we can add a all-0 column to that month\n",
    "        check_list = list(self.month_list - set(dummy_month.columns))\n",
    "        if len(check_list) != 0:\n",
    "            for col in check_list:\n",
    "                print(\"{} month does not exist in the current data, append column with all 0\".format(col))\n",
    "                dummy_month.loc[:, col] = 0\n",
    "               \n",
    "        # add prefix to dummy month and combine it with original data\n",
    "        dummy_month = dummy_month.add_prefix(\"month_\")\n",
    "        new_X = new_X.drop(columns=[\"month\"])\n",
    "        new_X = pd.concat([new_X, dummy_month], axis = 1)\n",
    "        \n",
    "        # apply target encoding value to weekofyear\n",
    "        new_X.loc[:, \"weekofyear\"] = new_X.loc[:, \"weekofyear\"].map(self.target_encoder_dict)\n",
    "        \n",
    "        print(\"Transforming data using Cat_encoder completed\")\n",
    "        \n",
    "        return new_X\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04595613",
   "metadata": {},
   "source": [
    "# Stationarity Adjustment\n",
    "\n",
    "A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary â€” the trend and seasonality will affect the value of the time series at different times.\n",
    "\n",
    "A non stationary feature, such as the feature value continuously grows or its variance grows over time, will lead to serious prediction error, especially in the future data.\n",
    "\n",
    "In this dataset, all the originla predictors are time-series feature. Thus, it is important to check the stationarity. If stationarity check failed, we could apply differencing method to make a non-stationary time series stationary \n",
    "\n",
    "reference: https://otexts.com/fpp2/stationarity.html\n",
    "\n",
    "Based on the data exploration analysis, I did not observes any siginifcant feature value growth over time. However, some predictors seems to have little unconditional variance that grows over time. This will lead to a condition called unit-root. One method of test unit-root is Agumented Dickey Fuller test. During the test, if we obtain a p-value less than 0.05, we could safely reject the null hypothesis(null hypothesis: the feature suffers from a unit-root stationary). Otherwise, we could apply differecing method to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32378f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stationarity_adjustment(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class serves three functions:\n",
    "    1. Stationarity check through ADF\n",
    "    2. Remove unit-root using differecning method\n",
    "    3. Check the input data and adjust the data based on function 1 and 2\n",
    "    \n",
    "    This class will only apply to the original columns, then encoded columns like\n",
    "    month_ and weekofyear will be ingored automatically\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, p_value = 0.05):\n",
    "        '''\n",
    "        p_value: the siginficance level for adf test\n",
    "        '''\n",
    "        self.p_value = p_value\n",
    "        pass\n",
    "\n",
    "    def stationarity_check_adf(self, predictor):\n",
    "        '''\n",
    "        stationary check using adf method\n",
    "        \n",
    "        input: predictor - pandas series:the column of the selected feature\n",
    "        output: adf_checker - bool: If ture, reject the null hypothesis\n",
    "                              the predictor does not suffer unit-root stationary\n",
    "        '''\n",
    "        # remove missing value\n",
    "        temp_predictor = predictor.dropna()\n",
    "        # apply adf test\n",
    "        adf_checker = adfuller(x=temp_predictor, \n",
    "                                  regression=\"c\", \n",
    "                                  autolag=\"AIC\")[1] < self.p_value\n",
    "        return adf_checker\n",
    "\n",
    "    def differencing(self, predictor, adf_checker):\n",
    "        '''\n",
    "        adjust the predicor who suffers the unit-root stationary using differencing until pass the adf test\n",
    "        \n",
    "        input: predictor - pandas series:the column of the selected feature\n",
    "               adf_checker - bool: If ture, reject the null hypothesis\n",
    "                              the predictor does not suffer unit-root stationary\n",
    "        '''\n",
    "        predictor.dropna(inplace=True)\n",
    "        # apply adifferencing until it pass the adf test\n",
    "        while not adf_checker:\n",
    "            predictor = predictor.diff()\n",
    "            adf_checker = self.stationarity_check_adf(predictor)\n",
    "            \n",
    "        return predictor\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        stationarity check on all original columns\n",
    "        Ingore the month encoding columns and weekofyear column since they are encoded column\n",
    "        if check failed, apply differencing\n",
    "        \n",
    "        If the data does not have enough obs, adf test may failed, in that case, we skip. This \n",
    "        will not happen as long as we carefully split the data\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            print(\"checking stationarity\")\n",
    "            original_columns = [x for x in X.columns if not x.startswith(\"month_\") and \"weekofyear\" not in x]\n",
    "            for col in original_columns:\n",
    "                predictor = X.loc[:, col].copy()\n",
    "                adf_checker = self.stationarity_check_adf(predictor)\n",
    "                if not adf_checker:\n",
    "                    print(\"{} failed the stationarity check, applying differencing\".format(col))\n",
    "                    predictor = self.differencing(predictor, adf_checker)\n",
    "                \n",
    "                X.loc[:, col] = predictor.copy()\n",
    "            \n",
    "            print(\"Checking stationarity completed\")\n",
    "                \n",
    "        except ValueError as e:\n",
    "            print(\"ERROR!:{}\".format(e))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765c50c",
   "metadata": {},
   "source": [
    "# Feature argumentation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f69c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_argumentation1(BaseEstimator, TransformerMixin):\n",
    "    def __int__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        print(\"Argumenting feature - part 1\")\n",
    "        output_X = X.copy()\n",
    "        \n",
    "        # ndvi std between ne, nw, se and sw\n",
    "        temp_X = X.loc[:, [\"ndvi_ne\", \"ndvi_nw\", \"ndvi_se\", \"ndvi_sw\"]].copy()\n",
    "        output_X.loc[:, \"fa_ndvi_std\"] = temp_X.std(axis = 1)\n",
    "        \n",
    "        # create: range in station_max_temp\n",
    "        #         skewness in stationa temperature\n",
    "        temp_X = X.loc[:,[\"station_max_temp_c\", \n",
    "                          \"station_min_temp_c\", \n",
    "                          \"station_avg_temp_c\"]].copy()\n",
    "        \n",
    "        output_X.loc[:, \"fa_station_temp_range_c\"] = temp_X[\"station_max_temp_c\"] - temp_X[\"station_min_temp_c\"]        \n",
    "        output_X.loc[:, \"fa_station_temp_skew_c\"] = (temp_X[\"station_max_temp_c\"] + temp_X[\"station_min_temp_c\"])/2 - temp_X[\"station_avg_temp_c\"]\n",
    "        \n",
    "        # create: range in reanalysis temperature\n",
    "        #         skewness in reanalysis temperature\n",
    "        #         temp difference between average and dew point\n",
    "        temp_X = X.loc[:,[\"reanalysis_dew_point_temp_k\", \n",
    "                          \"reanalysis_air_temp_k\", \n",
    "                          \"reanalysis_max_air_temp_k\",\n",
    "                          \"reanalysis_min_air_temp_k\",\n",
    "                          \"reanalysis_avg_temp_k\"]].copy()\n",
    "        \n",
    "        output_X.loc[:, \"fa_reanalysis_temp_range_k\"] = temp_X[\"reanalysis_max_air_temp_k\"] - temp_X[\"reanalysis_min_air_temp_k\"]\n",
    "        output_X.loc[:, \"fa_reanalysis_temp_skew_k\"] = (temp_X[\"reanalysis_max_air_temp_k\"] + temp_X[\"reanalysis_min_air_temp_k\"])/2 - temp_X[\"reanalysis_avg_temp_k\"]\n",
    "        output_X.loc[:, \"fa_reanalysis_dew_diff_k\"] = temp_X[\"reanalysis_avg_temp_k\"] - temp_X[\"reanalysis_dew_point_temp_k\"]\n",
    "        \n",
    "        print(\"Argumenting feature - part 1 completed\")\n",
    "        \n",
    "        return output_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b65522",
   "metadata": {},
   "source": [
    "# Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d6b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Standardizing the data and return the standarzed dataframe\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.column_names = None\n",
    "        self.fitted_standarizer = None       \n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"Fitting Standarizer\")\n",
    "        self.column_names = X.columns\n",
    "        scaler = StandardScaler()\n",
    "        self.fitted_standarizer = scaler.fit(X)\n",
    "        print(\"Fitting Standarizer Completed\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # ---------------------input check-------------------------------                \n",
    "        if self.column_names is None or self.fitted_standarizer is None:\n",
    "            raise ValueError(\"Please fit the standardizer first\")\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        print(\"Applying Standarizer\")\n",
    "        X_scaled = self.fitted_standarizer.transform(X)\n",
    "        X_scaled_df = pd.DataFrame(data=X_scaled, columns=self.column_names)\n",
    "        print(\"Applying Standarizer Completed\")\n",
    "        \n",
    "        return X_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a651e3c",
   "metadata": {},
   "source": [
    "# Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6ebf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class is used to impute mising values\n",
    "    We have two mode:\n",
    "        mode 0: impute missing value using linear interpolate, since the original features are time series based\n",
    "        mode 1: impute missing value using KNN imputer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impute_mode = 1, n_neighbors = 10):\n",
    "        '''\n",
    "        impute_mode: 1/0 binary flag\n",
    "        n_nenighbours: int, the number of neighbors used in KNN imputer\n",
    "        '''\n",
    "        self.impute_mode = impute_mode\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.columns_names = None\n",
    "        self.fitted_imputer = None\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.impute_mode == 1:\n",
    "            print(\"Fitting KNN imputer\")\n",
    "            self.columns_names = X.columns\n",
    "            impute = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "            self.fitted_imputer = impute.fit(X)          \n",
    "            print(\"Fitting KNN imputer Completed\")\n",
    "        else:\n",
    "            print(\"Linear interpolatation imputation method will be used. No fitting needed\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # ---------------------input check-------------------------------                \n",
    "        if self.impute_mode == 1 and (self.columns_names is None or self.fitted_imputer is None):\n",
    "            raise ValueError(\"Please fit the KNN imputer first\")\n",
    "        #----------------------------------------------------------------\n",
    "        print(\"Imputing Missing Values\")\n",
    "        if self.impute_mode == 0:\n",
    "            imputed_X_df = X.interpolate()\n",
    "        else:\n",
    "            imputed_X = self.fitted_imputer.transform(X)\n",
    "            imputed_X_df = pd.DataFrame(data=imputed_X, columns=self.columns_names)\n",
    "        print(\"Imputing Missing Values completed\")\n",
    "        \n",
    "        return imputed_X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9515348",
   "metadata": {},
   "source": [
    "# Feature argumentaion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c6b24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_argumentation2(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    - This class is used for feature argumentation part 2.\n",
    "    - In this part, gradient and drift features are created\n",
    "      - gradient feature: the gradient of the original feature based on a specified time period\n",
    "      - drift feature: simply move the feature value forward based on a specified time period\n",
    "    - the time period is pre-set as 4 weeks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mode = 1, max_t_drift = 4):\n",
    "        '''\n",
    "        input: mode - 1/0 flag, a swtich to determine whether to use this argumentation step\n",
    "               max_t_drift: int, specified time period\n",
    "               drift_data: the drift data used to append on test set\n",
    "        '''\n",
    "        self.mode = mode\n",
    "        self.max_t_drift = max_t_drift\n",
    "        self.drift_data = None\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        '''\n",
    "        store the drift data\n",
    "        '''\n",
    "        self.drift_data = X.tail(self.max_t_drift)\n",
    "        return self\n",
    "    \n",
    "    def _fitcheck(self, X):\n",
    "        '''\n",
    "        - This function is used to check whether the transform is applied to the train data or not.\n",
    "        - If the tail of input data is not identical to the drift data, the current input data is the test set, return True.\n",
    "        - Otherwise, return False\n",
    "        '''\n",
    "        \n",
    "        if self.drift_data is not None and (self.drift_data != X.tail(self.max_t_drift)).sum().sum() > 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def _cal_gradient(self, original_X, X, t_drift):\n",
    "        '''\n",
    "        - This function is used to create the gradient features\n",
    "        - input: original_X - dataframe, the original input dataframe. Used in _fitcheck only\n",
    "                 X - dataframe, the original dataframe + the drift data. Used to derive gradient feature\n",
    "                 t_drift - int, the specificed time window\n",
    "        '''\n",
    "        \n",
    "        temp_X = X.copy()\n",
    "        temp_X.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        # create a dictionary to store the gradient features\n",
    "        out_X = collections.defaultdict(list)\n",
    "        \n",
    "        for col in temp_X.columns:\n",
    "            # the name of gradient feature comes with prefix gradient_\n",
    "            new_col_name = \"gradient_\" + str(t_drift) + \"_\" + col\n",
    "            # create a empty list to store the gradient for each obs\n",
    "            temp_gradient = []\n",
    "            for i in range(t_drift, temp_X.shape[0]):\n",
    "                # since the time window is small, we simply use least square method\n",
    "                # to get the gradient\n",
    "                x = np.arange(0, t_drift)\n",
    "                A = np.vstack([x, np.ones(len(x))]).T\n",
    "                y = temp_X.loc[:, col].to_numpy()\n",
    "                y = y[i - t_drift:i]\n",
    "                m, c = np.linalg.lstsq(A, y, rcond = None)[0]\n",
    "                # store the gradient to temp list\n",
    "                temp_gradient.append(m)\n",
    "            \n",
    "            # for the first t_drift obs, we can not calcualte gradient\n",
    "            # thus use bfill method to assign gradient value\n",
    "            out_X[new_col_name] = [temp_gradient[0]]*t_drift + temp_gradient\n",
    "        \n",
    "        # conver to dataframe\n",
    "        out_X = pd.DataFrame(out_X)        \n",
    "        out_columns = out_X.columns\n",
    "        \n",
    "        # Get rid of the appended drift data\n",
    "        out_array = out_X.to_numpy()[self.max_t_drift:,:]\n",
    "        \n",
    "        # If we are dealing with train data, bfill the feature values in time winodws\n",
    "        if not self._fitcheck(original_X):\n",
    "            temp_array = np.repeat(out_array[[t_drift],:], t_drift, axis = 0)\n",
    "            out_array = np.row_stack((temp_array, out_array[t_drift:, :]))\n",
    "        \n",
    "                \n",
    "        return out_array, out_columns\n",
    "    \n",
    "    \n",
    "    def _drift_feature(self, original_X, X, t_drift):\n",
    "        '''\n",
    "        - This function is used to create the drift features\n",
    "        - input: original_X - dataframe, the original input dataframe. Used in _fitcheck only\n",
    "                 X - dataframe, the original dataframe + the drift data. Used to derive gradient feature\n",
    "                 t_drift - int, the specificed time window\n",
    "        '''\n",
    "        temp_X = X.copy()\n",
    "        temp_X.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        # create a dictionary to store the gradient features\n",
    "        out_X = collections.defaultdict(list)\n",
    "        \n",
    "        for col in temp_X.columns:\n",
    "            # the name of drift feature comes with prefix drift_\n",
    "            new_col_name = \"drift_\" + str(t_drift) + \"_\" + col\n",
    "            out_X[new_col_name] = list(temp_X.loc[:, col])\n",
    "        \n",
    "        # conver to dataframe\n",
    "        out_X = pd.DataFrame(out_X)\n",
    "        out_columns = out_X.columns\n",
    "        \n",
    "        # Get rid of the appended drift data\n",
    "        start_index = self.max_t_drift - t_drift\n",
    "        end_index = out_X.shape[0] - t_drift\n",
    "        out_array = out_X.to_numpy()[start_index:end_index,:]\n",
    "\n",
    "        # If we are dealing with train data, bfill the feature values in time winodws\n",
    "        if not self._fitcheck(original_X):\n",
    "            temp_array = np.repeat(out_array[[t_drift],:], t_drift, axis = 0)\n",
    "            out_array = np.row_stack((temp_array, out_array[t_drift:, :]))\n",
    "        \n",
    "        \n",
    "        return out_array, out_columns\n",
    "            \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        output_X = X.copy()\n",
    "        \n",
    "        # the drift features does not contains encoded feature\n",
    "        drift_columns = [x for x in X.columns if not x.startswith(\"month_\") and \"weekofyear\" not in x]\n",
    "        # the gradient features must be derived from original predictors\n",
    "        gradient_columns = [x for x in drift_columns if not x.startswith(\"fa_\")]\n",
    "        \n",
    "        drift_X = X.loc[:, drift_columns].copy()\n",
    "        gradient_X = X.loc[:, gradient_columns].copy()\n",
    "        \n",
    "        if self.mode == 1:\n",
    "            print(\"Argumenting feature - part 2\")\n",
    "            temp_drift_data = self.drift_data.loc[:, drift_columns].copy()\n",
    "            temp_gradient_data = self.drift_data.loc[:, gradient_columns].copy()\n",
    "            drift_X = pd.concat([temp_drift_data, drift_X], ignore_index = True)\n",
    "            gradient_X = pd.concat([temp_gradient_data, gradient_X], ignore_index = True)\n",
    "            \n",
    "            # iterate from 1 to the max drift window\n",
    "            # create drifted features and gradient features for each drift time window size\n",
    "            for i in range(1, self.max_t_drift + 1):\n",
    "                out_array1, out_columns1 = self._drift_feature(X, drift_X, i)\n",
    "                out_array2, out_columns2 = self._cal_gradient(X, gradient_X, i)\n",
    "                output_X.loc[:, out_columns1] = out_array1\n",
    "                output_X.loc[:, out_columns2] = out_array2\n",
    "            \n",
    "            print(\"Argumenting feature - part 2 completed\")\n",
    "        \n",
    "        return output_X          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0021a64",
   "metadata": {},
   "source": [
    "# Feature Argumentation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3598c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_argumentation3(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "     - This feature argumentation step will add polynomial features\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mode = 1, degree = 2):\n",
    "        '''\n",
    "        - input: mode - 1/0 flag, a swticher to determine whether to use argumentation part 3\n",
    "                 degree - int, the degree of polynomial       \n",
    "        '''\n",
    "        self.mode = mode\n",
    "        self.degree = degree\n",
    "        self.fitted_standarizer = None\n",
    "        pass\n",
    "    \n",
    "    def _polyfeatures(self, X):\n",
    "        '''\n",
    "        This function will apply polynomial transformation to create polynomials based on the original variables\n",
    "        '''\n",
    "        # select the columns to be included in the polynominal transformation\n",
    "        poly_columns = [x for x in X.columns if not x.startswith(\"month_\") and \"weekofyear\" not in x]\n",
    "        poly_columns = [x for x in poly_columns if not x.startswith(\"drift_\") and not x.startswith(\"gradient_\")]\n",
    "        poly_data = X.loc[:, poly_columns].copy()\n",
    "        \n",
    "        # apply polynominal transformation\n",
    "        poly = PolynomialFeatures(degree=self.degree, include_bias=False, interaction_only=False)\n",
    "        poly_array = poly.fit_transform(poly_data)\n",
    "        \n",
    "        # save the generated feature into a dataframe\n",
    "        poly_feature_names = poly.get_feature_names_out(poly_columns)\n",
    "        poly_data_df = pd.DataFrame(data=poly_array, columns=poly_feature_names)\n",
    "        poly_wo_old_feature = poly_data_df.loc[:, [x for x in poly_feature_names if x not in poly_columns]]\n",
    "        return poly_wo_old_feature\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        if self.mode == 1:\n",
    "            print(\"Fitting Standarizer for argumented feature in part 3\")\n",
    "            # To make sure the generated features are standarized. we create a standardscaler here\n",
    "            # and save it in the obejct\n",
    "            poly_wo_old_feature = self._polyfeatures(X)\n",
    "            scaler = StandardScaler()\n",
    "            self.fitted_standarizer = scaler.fit(poly_wo_old_feature)\n",
    "            print(\"Fitting Standarizer for argumented feature in part 3 Completed\")\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.mode == 1:\n",
    "            # ---------------------input check-------------------------------                \n",
    "            if self.fitted_standarizer is None:\n",
    "                raise ValueError(\"Please fit this object first.\")\n",
    "            #----------------------------------------------------------------\n",
    "            print(\"Argumenting feature - part 3\")\n",
    "            # obtain the polynomial features\n",
    "            poly_wo_old_feature = self._polyfeatures(X)\n",
    "            temp_columns = poly_wo_old_feature.columns\n",
    "            \n",
    "            # apply the standardizer and add the new features to original input data\n",
    "            poly_wo_old_feature = self.fitted_standarizer.transform(poly_wo_old_feature)\n",
    "            poly_wo_old_feature = pd.DataFrame(data=poly_wo_old_feature, \n",
    "                                               columns=temp_columns)\n",
    "            \n",
    "            X = pd.concat([X, poly_wo_old_feature], axis=1)\n",
    "            \n",
    "            print(\"Argumenting feature - part 3 completed\")\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9532105",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a420c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_selection(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    - This class is based on paper https://ieeexplore.ieee.org/abstract/document/8871132\n",
    "    - linear model, svm model and random forest model will be used to estimate the feature\n",
    "      importances.\n",
    "    - For each model, ranked the feature by feature importance and select the top X% importance\n",
    "      feature. Then we treate each feature as the row, each obs as the columns and apply clustering.\n",
    "      If two features are similar in vector space. Then these two features will be selected by the \n",
    "      same cluster. For each cluster we pick up the most importance feature.\n",
    "    - Affinity Propagation clustering is used. It is a heirical clustering method which does not\n",
    "      need to specify the number of clusters: https://www.toptal.com/machine-learning/clustering-algorithms\n",
    "    - Three models will generate three importance feature lists. Combine all selected feature together as\n",
    "      the final list\n",
    "    - In order to determine X%, cross validation is used to evaluate the selected feature on the final\n",
    "      estimator. Then, the X% leads to the best cv score will be used as the final feature selection fraction\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 feature_frac_list = [0.01, 0.05, 0.1, 0.25, 0.5], \n",
    "                 cv_metric = \"r2\", \n",
    "                 classify_mode = 0, \n",
    "                 estimator=None):\n",
    "        # a list of feature fraction that will be explored\n",
    "        self.feature_frac_list = feature_frac_list\n",
    "        # the cv metric used to determine the best feature fraction\n",
    "        self.cv_metric = cv_metric\n",
    "        # 1/0 flag, a swtich to determine if feature selection is for a classfication task or not\n",
    "        self.classify_mode = classify_mode\n",
    "        # the estimator object for the final model\n",
    "        self.estimator = estimator\n",
    "        # A list of all input variable names\n",
    "        self.X_columns = None\n",
    "        # A list of the selected best features\n",
    "        self.best_features = None\n",
    "        pass\n",
    "\n",
    "    def _importance_df_creator(self, importances):\n",
    "        '''\n",
    "        - create a dataframe to store the feature importance\n",
    "        - input: importances - array, store feature importance or coefficient\n",
    "        - output: importance_df - dataframe, store feature names, importances and rank.\n",
    "                                this dataframe is ranked by importance in descending order\n",
    "        '''\n",
    "        # create a dataframe with feature names and their importances\n",
    "        importances_df = pd.DataFrame({\"feature_names\": self.X_columns, \n",
    "                                       \"importances\": importances})\n",
    "        # get the rank based on importances, rank 1 means the most importance\n",
    "        # it is possible that some variables share the same rank\n",
    "        importances_df.loc[:, \"rank\"] = importances_df.loc[:, \"importances\"].rank(method=\"dense\", \n",
    "                                                                                  ascending=False)\n",
    "        # sort the dataframe by rank\n",
    "        importances_df.sort_values(by=\"rank\", \n",
    "                                   inplace=True)\n",
    "        return importances_df\n",
    "\n",
    "    def _coef_feature_selection(self, model, X, y):\n",
    "        '''\n",
    "        - fitting a baseline model and output the feature importance dataframe\n",
    "        - input: model - model object\n",
    "                 X - input variables\n",
    "                 y - target variables\n",
    "        - output: importances_df - dataframe, feature importance dataframe\n",
    "        '''\n",
    "        # if the model used to determine feature importance is based on coefficient\n",
    "        # we fit the model and get the coefficients\n",
    "        model.fit(X, y)\n",
    "        model_coef = abs(model.coef_)\n",
    "        # some model output a 1-dimension coef, some model output a (1,X) shape coefficient\n",
    "        # reshape the coefficent array to make sure it is only 1-dimension\n",
    "        model_coef = model_coef.reshape(-1)\n",
    "        # create the feature importance dataframe\n",
    "        importances_df = self._importance_df_creator(model_coef)\n",
    "        return importances_df\n",
    "\n",
    "    def _non_coef_feature_selection(self, model, X, y):\n",
    "        '''\n",
    "        - fitting a baseline model and output the feature importance dataframe\n",
    "        - input: model - model object\n",
    "                 X - input variables\n",
    "                 y - target variables\n",
    "        - output: importances_df - dataframe, feature importance dataframe\n",
    "        '''\n",
    "        # for tree-based model, feature importances are obtained from\n",
    "        # feature_importnaces_\n",
    "        model.fit(X, y)\n",
    "        model_fi = model.feature_importances_\n",
    "        # get feature importance dataframe\n",
    "        importances_df = self._importance_df_creator(model_fi)\n",
    "        return importances_df\n",
    "\n",
    "    def _clustering_features(self, df1, df2, df3, X):\n",
    "        '''\n",
    "        - Apply affinity propagation clustering to get the top features\n",
    "        - input: df1 - dataframe, importance df based on svm\n",
    "                 df2 - dataframe, importance df based on linear model\n",
    "                 df3 - dataframe, importance df based on random forest\n",
    "                 X - dataframe, input data\n",
    "        - output: unique_relevant_features_list - list, a list of relevant features\n",
    "        '''\n",
    "        # a list to store selected relevant features\n",
    "        relevant_feature_list = []\n",
    "        \n",
    "        cluster_algo = AffinityPropagation(random_state=42, max_iter=2500)\n",
    "        \n",
    "        # based on feature rankings determined by each model\n",
    "        # select the relevant features based on clustering result\n",
    "        for df in [df1, df2, df3]:\n",
    "            subset_df = df.iloc[:self.n_keep, :].copy()\n",
    "            chosen_features = subset_df.loc[:, \"feature_names\"].tolist()\n",
    "            subset_feature_data = X.loc[:, chosen_features]\n",
    "            feature_cluster = cluster_algo.fit_predict(subset_feature_data.T)\n",
    "            subset_df.loc[:, \"cluster\"] = feature_cluster\n",
    "            \n",
    "            # iterate through each cluster and select the most relevant feature\n",
    "            # in each cluster\n",
    "            for cluster in set(feature_cluster):\n",
    "                cluster_df = subset_df.loc[subset_df[\"cluster\"] == cluster, :]\n",
    "                lowest_rank = cluster_df.loc[:, \"rank\"].min()\n",
    "                relevant_features = cluster_df.loc[cluster_df[\"rank\"] == lowest_rank, :].loc[:, \"feature_names\"].tolist()\n",
    "                relevant_feature_list += relevant_features\n",
    "        \n",
    "        # get rid of the overlapping features\n",
    "        unique_relevant_features_list = list(set(relevant_feature_list))\n",
    "        return unique_relevant_features_list\n",
    "\n",
    "    def _adjusted_R2_score(self, R2_cv_scores, nfeatures, nobs):\n",
    "        '''\n",
    "        A function to calculate adjusted R2 score\n",
    "        '''\n",
    "        scores = np.mean(R2_cv_scores)\n",
    "        adj_score = 1 - ((1-scores) * (nobs-1) / (nobs-nfeatures-1))\n",
    "        return adj_score\n",
    "\n",
    "    def _choose_best_features(self, X, y):\n",
    "        '''\n",
    "        - A wrapper function to select the best relevant features\n",
    "        '''\n",
    "        cv = 3\n",
    "        nobs = len(X)\n",
    "        list_scoring_results, list_selected_features = [], []\n",
    "        \n",
    "        # create the estimator object\n",
    "        if self.classify_mode == 1:\n",
    "            svm_model = svm.SVC(kernel=\"linear\", random_state=42)\n",
    "            linear_model = LogisticRegression(max_iter=5_000, random_state=42)\n",
    "            rfr_model = RandomForestClassifier(random_state=42)\n",
    "            if self.cv_metric == \"r2\":\n",
    "                raise ValueError(\"Please assign a classification metric to cv_metric. The default R2 is for regression\")\n",
    "        else:\n",
    "            svm_model = svm.SVR(kernel=\"linear\")\n",
    "            linear_model = Ridge()\n",
    "            rfr_model = RandomForestRegressor(random_state=42)\n",
    "            \n",
    "            # since the max cv_metric value will be selected to determine the final features\n",
    "            # restric regression metric to r2\n",
    "            if self.cv_metric != \"r2\":\n",
    "                print(\"Swtiched cv_metric for R2\")\n",
    "                self.cv_metric = \"r2\"\n",
    "\n",
    "        svm_columns_df = self._coef_feature_selection(svm_model, X, y)\n",
    "        lgr_columns_df = self._coef_feature_selection(linear_model, X, y)\n",
    "        rfr_columns_df = self._non_coef_feature_selection(rfr_model, X, y)\n",
    "        \n",
    "        # iterate through each feature fraction, evaluate feature selection through cv\n",
    "        for feature_frac in self.feature_frac_list:\n",
    "            self.n_keep = int(len(self.X_columns) * feature_frac)\n",
    "            selected_features = self._clustering_features(svm_columns_df, lgr_columns_df, rfr_columns_df, X=X)\n",
    "\n",
    "            list_selected_features.append(selected_features)\n",
    "            subset_X = X.loc[:, selected_features].copy()\n",
    "            cv_scores = cross_val_score(self.estimator, subset_X, y, cv=cv, scoring=self.cv_metric)\n",
    "\n",
    "            nfeatures = len(subset_X.columns)\n",
    "            if self.cv_metric == \"r2\":\n",
    "                adj_score = self._adjusted_R2_score(cv_scores, nfeatures, nobs)\n",
    "            else:\n",
    "                adj_score = cv_scores\n",
    "                \n",
    "            list_scoring_results.append(adj_score)\n",
    "        \n",
    "        # select the best feature set based on cv score\n",
    "        max_index = np.argmax(list_scoring_results)\n",
    "        best_features = list_selected_features[max_index]\n",
    "        \n",
    "        print(\"Selecting features: the best fraction is {}\".format(self.feature_frac_list[max_index]))\n",
    "        \n",
    "        return best_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"Selecting features\")\n",
    "        self.X_columns = X.columns\n",
    "        self.best_features = self._choose_best_features(X, y)\n",
    "        print(\"Selecting features completed\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # ---------------------input check-------------------------------                \n",
    "        if self.best_features is None:\n",
    "                raise ValueError(\"Feature has not been selected based on training data\")\n",
    "        #----------------------------------------------------------------        \n",
    "        subset_X = X.loc[:, self.best_features]\n",
    "        \n",
    "        print(\"Applied selected features\")\n",
    "        \n",
    "        return subset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da530a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
